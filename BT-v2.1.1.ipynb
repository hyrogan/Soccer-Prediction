{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.399880Z",
     "start_time": "2023-07-18T16:36:18.384100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "def train_model(df):\n",
    "    # Convert match_outcome from categorical to numeric\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['match_outcome_encoded'] = label_encoder.fit_transform(df['match_outcome'])\n",
    "\n",
    "    # Create Logistic Regression model\n",
    "    model = LogisticRegression(multi_class='ovr')  # 'ovr' stands for One-Vs-Rest\n",
    "\n",
    "    # Reshape rating_difference to 2D array for model fitting\n",
    "    X = df['rating_difference'].values.reshape(-1, 1)\n",
    "    y = df['match_outcome_encoded']\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model and label encoder\n",
    "    joblib.dump(model, 'model.pkl')\n",
    "    joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f'Model accuracy: {score*100:.2f}%')\n",
    "\n",
    "    return model, label_encoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.412551Z",
     "start_time": "2023-07-18T16:36:18.393779Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "# Function to initialize ratings\n",
    "def initialize_ratings(df_results, df_fixtures):\n",
    "    ratings = {}\n",
    "\n",
    "    # Iterate over teams in the results data\n",
    "    teams = set(df_results['home_team']).union(set(df_results['away_team'])).union(set(df_fixtures['home_team'])).union(set(df_fixtures['away_team']))\n",
    "    for team in teams:\n",
    "        # Initialize ratings for each team\n",
    "        ratings[team] = {\n",
    "            'brH': 0.0,\n",
    "            'brA': 0.0,\n",
    "            'continuous_overunderperformances': 0\n",
    "        }\n",
    "    print('Teams:', teams, end='\\n\\n')\n",
    "    return ratings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.412845Z",
     "start_time": "2023-07-18T16:36:18.401949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "# Function to update ratings based on results data\n",
    "def update_ratings(df_results, ratings):\n",
    "\n",
    "    #lambda: Determines to what extent the new match results influence the team ratings (could be improved to include temporal difference between matches)\n",
    "    learning_rate_lambda = 0.054\n",
    "\n",
    "    #psi: diminish the impact each additional goal difference error has on team ratings\n",
    "    diminishing_function_psi = lambda error: 3 * np.log10(1 + error)\n",
    "\n",
    "    #gamma: determines to what extent performances at the home grounds influence away team ratings and vice versa\n",
    "    learning_rate_gamma = 0.79\n",
    "\n",
    "    #phi: Represents the number of continuous performances, above or below expectations, which do not trigger the form factor\n",
    "    form_threshold_phi = 1\n",
    "\n",
    "    #mu: represents the rating difference used to establish provisional ratings from background ratings\n",
    "    rating_impact_mu = 0.01\n",
    "\n",
    "    #delta: the level by which rating impact μ diminishes with each additional continuous over/under-performance\n",
    "    diminishing_factor_delta = 2.5\n",
    "\n",
    "    # Iterate over each match in the results data\n",
    "    for index, row in df_results.iterrows():\n",
    "        print(\"Game Nr:\", index)\n",
    "\n",
    "        if index == 2000:\n",
    "            break\n",
    "\n",
    "        home_team = row['home_team']\n",
    "        away_team = row['away_team']\n",
    "        print(\"home_team:\", home_team)\n",
    "        print(\"away_team:\", away_team)\n",
    "\n",
    "        # Calculate home team rating\n",
    "        home_rating_x = ratings[home_team]['brH']\n",
    "        away_rating_x = ratings[home_team]['brA']\n",
    "        if (abs(ratings[home_team]['continuous_overunderperformances']) > form_threshold_phi):\n",
    "            home_rating_x = ratings[home_team]['prH']\n",
    "            away_rating_x = ratings[home_team]['prA']\n",
    "\n",
    "        # Calculate away team rating\n",
    "        away_rating_y = ratings[away_team]['brA']\n",
    "        home_rating_y = ratings[away_team]['brH']\n",
    "        if (abs(ratings[away_team]['continuous_overunderperformances']) > form_threshold_phi):\n",
    "            away_rating_y = ratings[away_team]['prA']\n",
    "            home_rating_y = ratings[away_team]['prH']\n",
    "\n",
    "\n",
    "\n",
    "        observed_goal_difference = row['home_goals'] - row['away_goals']\n",
    "        print(\"observed_goal_difference:\", observed_goal_difference)\n",
    "\n",
    "        #Calculate expected goals for home team\n",
    "        #expected_goal_x = round((10 ** (abs(ratings[home_team]['brH']) / 3)) - 1,5)\n",
    "        expected_goal_x_temp = abs(home_rating_x) / 3\n",
    "        expected_goal_x = np.power(10, expected_goal_x_temp) - 1\n",
    "        print(\"expected_goal_x:\", expected_goal_x)\n",
    "\n",
    "        # Calculate expected goals for away team\n",
    "        #expected_goal_y = round((10 ** (abs(ratings[away_team]['brA']) / 3)) - 1,5)\n",
    "        expected_goal_y_temp = abs(away_rating_y) / 3\n",
    "        expected_goal_y = np.power(10, expected_goal_y_temp) - 1\n",
    "        print(\"expected_goal_y:\", expected_goal_y)\n",
    "\n",
    "        # Calculate expected goal difference based on ratings\n",
    "        expected_goal_difference = expected_goal_x - expected_goal_y\n",
    "        print(\"expected_goal_difference:\", expected_goal_difference)\n",
    "\n",
    "\n",
    "        # Calculate the error between observed and expected goal difference\n",
    "        error = abs(observed_goal_difference - expected_goal_difference)\n",
    "        print(\"error:\", error)\n",
    "\n",
    "        psi_temp = diminishing_function_psi(error)\n",
    "\n",
    "        # Diminish the impact of the goal difference error for both teams x and y respectively\n",
    "        if (expected_goal_difference < observed_goal_difference):\n",
    "            diminishing_function_psi_x = psi_temp\n",
    "            diminishing_function_psi_y = -psi_temp\n",
    "        else:\n",
    "            diminishing_function_psi_x = -psi_temp\n",
    "            diminishing_function_psi_y = psi_temp\n",
    "        print(\"diminishing_function_psi_x:\", diminishing_function_psi_x)\n",
    "        print(\"diminishing_function_psi_y:\", diminishing_function_psi_y)\n",
    "\n",
    "        # Update the home team x background ratings\n",
    "        previous_home_rating_x = ratings[home_team]['brH']\n",
    "        previous_away_rating_x = ratings[home_team]['brA']\n",
    "        print(\"previous_home_rating_x:\", previous_home_rating_x)\n",
    "        print(\"previous_away_rating_x:\", previous_away_rating_x)\n",
    "\n",
    "        ratings[home_team]['brH'] = previous_home_rating_x + diminishing_function_psi_x * learning_rate_lambda\n",
    "        ratings[home_team]['brA'] = previous_away_rating_x + (ratings[home_team]['brH'] - previous_home_rating_x) * learning_rate_gamma\n",
    "        print(\"ratings[home_team]['brH']:\", ratings[home_team]['brH'])\n",
    "        print(\"ratings[home_team]['brA']:\", ratings[home_team]['brA'])\n",
    "\n",
    "        # Update the away team y background ratings\n",
    "        previous_home_rating_y = ratings[away_team]['brH']\n",
    "        previous_away_rating_y = ratings[away_team]['brA']\n",
    "        print(\"previous_home_rating_y:\", previous_home_rating_y)\n",
    "        print(\"previous_away_rating_y:\", previous_away_rating_y)\n",
    "\n",
    "        ratings[away_team]['brA'] = previous_away_rating_y + diminishing_function_psi_y * learning_rate_lambda\n",
    "        ratings[away_team]['brH'] = previous_home_rating_y + (ratings[away_team]['brA'] - previous_away_rating_y) * learning_rate_gamma\n",
    "        print(\"ratings[away_team]['brH']:\", ratings[away_team]['brH'])\n",
    "        print(\"ratings[away_team]['brA']:\", ratings[away_team]['brA'])\n",
    "\n",
    "        print(\"home team: previous overunderperformance:\", ratings[home_team]['continuous_overunderperformances'])\n",
    "        print(\"away team: previous overunderperformance:\", ratings[away_team]['continuous_overunderperformances'])\n",
    "\n",
    "        # Update the continuous over/underperformances for the home team\n",
    "        if (observed_goal_difference > expected_goal_difference):\n",
    "            ratings[home_team]['continuous_overunderperformances'] = max(1, ratings[home_team]['continuous_overunderperformances'] + 1)\n",
    "            ratings[away_team]['continuous_overunderperformances'] = min(-1, ratings[away_team]['continuous_overunderperformances'] - 1)\n",
    "        elif (observed_goal_difference < expected_goal_difference):\n",
    "            ratings[home_team]['continuous_overunderperformances'] = min(-1, ratings[home_team]['continuous_overunderperformances'] - 1)\n",
    "            ratings[away_team]['continuous_overunderperformances'] = max(1, ratings[away_team]['continuous_overunderperformances'] + 1)\n",
    "\n",
    "        print(\"home team: updated overunderperformance:\", ratings[home_team]['continuous_overunderperformances'])\n",
    "        print(\"away team: updated overunderperformance:\", ratings[away_team]['continuous_overunderperformances'], end='\\n\\n')\n",
    "\n",
    "\n",
    "        # Calculate performance factor for home team x\n",
    "        a = ratings[home_team]['continuous_overunderperformances'] - form_threshold_phi\n",
    "        b = a ** diminishing_factor_delta\n",
    "        if (a == 0):\n",
    "            form_factor_home = 0\n",
    "        else:\n",
    "            form_factor_home = a / b\n",
    "\n",
    "        # Calculate home team x provisional rating\n",
    "        home_rating = ratings[home_team]['brH']\n",
    "        if (ratings[home_team]['continuous_overunderperformances'] > form_threshold_phi):\n",
    "            home_rating = ratings[home_team]['brH'] + rating_impact_mu * form_factor_home\n",
    "        if (ratings[home_team]['continuous_overunderperformances'] < -form_threshold_phi):\n",
    "            home_rating = ratings[home_team]['brH'] - rating_impact_mu * form_factor_home\n",
    "\n",
    "        # Calculate performance factor for away team y\n",
    "        c = ratings[away_team]['continuous_overunderperformances'] - form_threshold_phi\n",
    "        d = c ** diminishing_factor_delta\n",
    "        if (c == 0):\n",
    "            form_factor_away = 0\n",
    "        else:\n",
    "            form_factor_away = c / d\n",
    "\n",
    "        # Calculate away team y provisional rating\n",
    "        away_rating = ratings[away_team]['brA']\n",
    "        if (ratings[away_team]['continuous_overunderperformances'] > form_threshold_phi):\n",
    "            away_rating = ratings[away_team]['brA'] + rating_impact_mu * form_factor_away\n",
    "        if (ratings[away_team]['continuous_overunderperformances'] < -form_threshold_phi):\n",
    "            away_rating = ratings[away_team]['brA'] - rating_impact_mu * form_factor_away\n",
    "\n",
    "\n",
    "    return ratings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.429604Z",
     "start_time": "2023-07-18T16:36:18.425272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "def calculate_provisional_ratings(ratings, team):\n",
    "\n",
    "    #phi: Represents the number of continuous performances, above or below expectations, which do not trigger the form factor\n",
    "    form_threshold_phi = 1\n",
    "\n",
    "    #mu: represents the rating difference used to establish provisional ratings from background ratings\n",
    "    rating_impact_mu = 0.01\n",
    "\n",
    "    #delta: the level by which rating impact μ diminishes with each additional continuous over/under-performance\n",
    "    diminishing_factor_delta = 2.5\n",
    "\n",
    "    brH = ratings[team]['brH']  # Background rating home\n",
    "    brA = ratings[team]['brA']  # Background rating away\n",
    "    prH = brH\n",
    "    prA = brA\n",
    "\n",
    "    # Calculate performance factor for home team x\n",
    "    a = ratings[team]['continuous_overunderperformances'] - form_threshold_phi\n",
    "    b = a ** diminishing_factor_delta\n",
    "    if (a == 0):\n",
    "        form_factor_home = 0\n",
    "    else:\n",
    "        form_factor_home = a / b\n",
    "\n",
    "    # Calculate home team x provisional rating\n",
    "    if (ratings[team]['continuous_overunderperformances'] > form_threshold_phi):\n",
    "        prH = brH + rating_impact_mu * form_factor_home\n",
    "        prA = brA + rating_impact_mu * form_factor_home\n",
    "    if (ratings[team]['continuous_overunderperformances'] < -form_threshold_phi):\n",
    "        prH = brH - rating_impact_mu * form_factor_home\n",
    "        prA = brA - rating_impact_mu * form_factor_home\n",
    "\n",
    "    return prH, prA"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.436702Z",
     "start_time": "2023-07-18T16:36:18.433802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "def calculate_probabilities(rating_difference):\n",
    "    # Load the model and label encoder\n",
    "    model = joblib.load('model.pkl')\n",
    "    label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "    # Now you can predict probabilities for a new game:\n",
    "    new_game_rating_difference = np.array([[rating_difference]])\n",
    "    probabilities = model.predict_proba(new_game_rating_difference)\n",
    "    decoded_predictions = {label: prob for label, prob in zip(label_encoder.classes_, probabilities[0])}\n",
    "\n",
    "    return decoded_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.461872Z",
     "start_time": "2023-07-18T16:36:18.440292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "# Function to calculate the rating difference between two teams\n",
    "def calculate_rating_difference(home_team, away_team, ratings):\n",
    "\n",
    "    # Calculate home team rating\n",
    "    home_rating_x = ratings[home_team]['brH']\n",
    "    if (abs(ratings[home_team]['continuous_overunderperformances']) > 1):\n",
    "        provisional_ratings_x = calculate_provisional_ratings(ratings, home_team)\n",
    "        home_rating_x = provisional_ratings_x[0]\n",
    "\n",
    "    # Calculate away team rating\n",
    "    away_rating_y = ratings[away_team]['brA']\n",
    "    if (abs(ratings[away_team]['continuous_overunderperformances']) > 1):\n",
    "        provisional_ratings_y = calculate_provisional_ratings(ratings, away_team)\n",
    "        away_rating_y = provisional_ratings_y[1]\n",
    "\n",
    "    # Calculate rating difference\n",
    "    rating_difference = home_rating_x - away_rating_y\n",
    "    print(rating_difference)\n",
    "\n",
    "    return rating_difference"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.462547Z",
     "start_time": "2023-07-18T16:36:18.448409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [],
   "source": [
    "def predict_outcomes(df_fixtures, ratings):\n",
    "    for index, row in df_fixtures.iterrows():\n",
    "        home_team = row['home_team']\n",
    "        away_team = row['away_team']\n",
    "        rating_difference = calculate_rating_difference(home_team, away_team, ratings)\n",
    "\n",
    "        home_win_prob, draw_prob, away_win_prob = calculate_probabilities(rating_difference)\n",
    "\n",
    "        print(f\"{home_team} - {away_team}: Outcome Predictions:\")\n",
    "        print(f\"Home Win: {home_win_prob}\")\n",
    "        print(f\"Draw: {draw_prob}\")\n",
    "        print(f\"Away Win: {away_win_prob}\", end='\\n\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.463405Z",
     "start_time": "2023-07-18T16:36:18.452893Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [],
   "source": [
    "def rps(probs, outcome):\n",
    "    cum_probs = np.cumsum(probs)\n",
    "    cum_outcomes = np.cumsum(outcome)\n",
    "\n",
    "    print(cum_outcomes)\n",
    "    print(cum_probs)\n",
    "    sum_rps = 0\n",
    "    for i in range(len(outcome)):\n",
    "        sum_rps+= (cum_probs[i] - cum_outcomes[i])**2\n",
    "\n",
    "    return sum_rps/(len(outcome)-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.468818Z",
     "start_time": "2023-07-18T16:36:18.464087Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load the results data file for seasons 2006-07 to 2016-17\n",
    "    df_results = pd.read_csv('../data/smallFixtures.csv')\n",
    "\n",
    "    # Load the fixtures data file for the season 2017-18\n",
    "    df_fixtures = pd.read_csv('../data/smallFixtures.csv')\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize ratings based on the results data\n",
    "    ratings = initialize_ratings(df_results, df_fixtures)\n",
    "\n",
    "    ratings['Leicester City']['brH'] = 0.463014\n",
    "    ratings['Leicester City']['brA'] = 0.208624\n",
    "    ratings['Leicester City']['continuous_overunderperformances'] = 3\n",
    "\n",
    "    ratings['Stoke City']['brH'] = 0.537708\n",
    "    ratings['Stoke City']['brA'] = 0.037819\n",
    "    ratings['Stoke City']['continuous_overunderperformances'] = -1\n",
    "\n",
    "\n",
    "    # Update ratings based on the results data\n",
    "    #ratings = update_ratings(df_results, ratings)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(df_train)\n",
    "\n",
    "    # Predict the probabilities of home win, draw and away win for the fixtures data\n",
    "    predict_outcomes(df_fixtures, ratings)\n",
    "\n",
    "    #ratings = update_ratings(df_results, ratings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Call Ranked Probability Score function\n",
    "    probs = [0.486, 0.261, 0.253]\n",
    "    outcome = [1, 0, 0]\n",
    "    rps_score = rps(probs, outcome)\n",
    "    print(\"RPS Score:\", rps_score, end='\\n\\n')\n",
    "    \"\"\"\n",
    "\n",
    "    for team, team_ratings in ratings.items():\n",
    "        print(f\"Team: {team}\")\n",
    "        print(f\"Background Rating Home: {team_ratings['brH']}\")\n",
    "        print(f\"Background Rating Away: {team_ratings['brA']}\")\n",
    "        print(f\"Continuous Over/Underperformances: {team_ratings['continuous_overunderperformances']}\")\n",
    "        print()\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation of the rating differences\n",
    "    rating_differences = calculate_rating_difference(df_results, ratings)\n",
    "    mean = np.mean(rating_differences)\n",
    "    std = np.std(rating_differences)\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Standard Deviation:\", std, end='\\n\\n')\n",
    "    \"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.485660Z",
     "start_time": "2023-07-18T16:36:18.475777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams: {'Stoke City', 'Leicester City'}\n",
      "\n",
      "0.42873053390593274\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[339], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[338], line 27\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m ratings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStoke City\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontinuous_overunderperformances\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Update ratings based on the results data\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#ratings = update_ratings(df_results, ratings)\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \n\u001B[1;32m     25\u001B[0m \n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Predict the probabilities of home win, draw and away win for the fixtures data\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m \u001B[43mpredict_outcomes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_fixtures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mratings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m#ratings = update_ratings(df_results, ratings)\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03m# Call Ranked Probability Score function\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03mprobs = [0.486, 0.261, 0.253]\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03mprint(\"RPS Score:\", rps_score, end='\\n\\n')\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[336], line 7\u001B[0m, in \u001B[0;36mpredict_outcomes\u001B[0;34m(df_fixtures, ratings)\u001B[0m\n\u001B[1;32m      4\u001B[0m away_team \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maway_team\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      5\u001B[0m rating_difference \u001B[38;5;241m=\u001B[39m calculate_rating_difference(home_team, away_team, ratings)\n\u001B[0;32m----> 7\u001B[0m home_win_prob, draw_prob, away_win_prob \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_probabilities\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrating_difference\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhome_team\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maway_team\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Outcome Predictions:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHome Win: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhome_win_prob\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[334], line 3\u001B[0m, in \u001B[0;36mcalculate_probabilities\u001B[0;34m(rating_difference)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculate_probabilities\u001B[39m(rating_difference):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# Load the model and label encoder\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mjoblib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     label_encoder \u001B[38;5;241m=\u001B[39m joblib\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel_encoder.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# Now you can predict probabilities for a new game:\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/soccer_v1/lib/python3.9/site-packages/joblib/numpy_pickle.py:650\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filename, mmap_mode)\u001B[0m\n\u001B[1;32m    648\u001B[0m         obj \u001B[38;5;241m=\u001B[39m _unpickle(fobj)\n\u001B[1;32m    649\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 650\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    651\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m _read_fileobject(f, filename, mmap_mode) \u001B[38;5;28;01mas\u001B[39;00m fobj:\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fobj, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    653\u001B[0m                 \u001B[38;5;66;03m# if the returned file object is a string, this means we\u001B[39;00m\n\u001B[1;32m    654\u001B[0m                 \u001B[38;5;66;03m# try to load a pickle file generated with an version of\u001B[39;00m\n\u001B[1;32m    655\u001B[0m                 \u001B[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'model.pkl'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T16:36:18.539674Z",
     "start_time": "2023-07-18T16:36:18.485286Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
